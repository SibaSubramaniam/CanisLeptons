{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlmodel.analysis import Analyser\n",
    "from mlmodel.cross_validation import Purged_validation, HyperParameterTuning\n",
    "from mlmodel.validation import Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from datagen import DataGen\n",
    "dg=DataGen()\n",
    "\n",
    "from visualize.visualize import Visualizer\n",
    "vl=Visualizer()\n",
    "\n",
    "from mlmodel.performanceMetrics import Metrics\n",
    "met_ob = Metrics()\n",
    "\n",
    "#model selection\n",
    "from mlmodel.split import Split\n",
    "from mlmodel.performanceMetrics import Metrics\n",
    "from mlmodel.mlclassfier import MLClassifier\n",
    "from mlmodel.sequential_bootstrap import sequentialBootstrap\n",
    "from mlmodel.analysis import Analyser\n",
    "from mlmodel.validation import Validation\n",
    "\n",
    "# For ML\n",
    "split_ob = Split()\n",
    "metrics_ob = Metrics()\n",
    "model_ob = MLClassifier()\n",
    "sb_ob = sequentialBootstrap()\n",
    "an_ob = Analyser()\n",
    "val_ob = Validation()\n",
    "\n",
    "val_ob = Validation()\n",
    "hpt_ob = HyperParameterTuning()\n",
    "pv_ob = Purged_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name='data/historical_price_data/BTCUSDT'\n",
    "bar_type='time'             #type of bars possible_values: dollar,time,ticks,volume\n",
    "threshold=300               #threshold for the given type of bar\n",
    "\n",
    "#normalization\n",
    "before=True                #flag that denotes normalizing before/after bars creation\n",
    "normalize=True             #flag that specifies whether normalization should be done\n",
    "norm_method='multiply'     #method for nomalization include 'multiply','min_max'\n",
    "norm_val=100               #threshold for the above mentioned method\n",
    "\n",
    "\n",
    "# Labels\n",
    "volatility_threshold=20    #threshold in bars for volatility which is standard deviation of returns\n",
    "sampling=False             #flag to control downsampling\n",
    "v_bars_duration=20           #threshold in bars for vertical_bars which denotes a dataframe in triple-barrier method\n",
    "barrier_conf=[2,4]          #stop loss and profit taking limits [0]denotes stop loss and [1]denotes profit taking\n",
    "min_return=0                #minimum values for return in triple-barrier method\n",
    "risk=0                      #risk for calculating sharp_ration\n",
    "sign_label=True             #flag to determine labels of vertical bars t1b\n",
    "\n",
    "# Features\n",
    "sma_period = [10, 20] # [10, 15, 20]\n",
    "ema_period = [10, 20] # [10, 15, 20]\n",
    "BB_period  = [15]\n",
    "rsi_period = [15]\n",
    "williamsr_period = [15]\n",
    "roc_period = [15]\n",
    "adl_period = [15]\n",
    "vpt_period = [0] # 0:  period is not required\n",
    "emv_period = [0] # 0:  period is not required\n",
    "\n",
    "feature_list = ['sma',      'ema',    'BB',       'rsi',     'williamsr',        'roc', \n",
    "                'adl',     'vpt',   'emv']   #feature list \n",
    "period_all =[sma_period, ema_period, BB_period, rsi_period, williamsr_period, roc_period, \n",
    "             adl_period, vpt_period, emv_period ]  # feature list period (change this if feature_list_changed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:  -1.0    28143\n",
      " 1.0    16019\n",
      " 0.0    12291\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "raw_data,labels,labels_features,train,test=dg.create_data(folder_name,feature_list,period_all,before,normalize,norm_val,norm_method,bar_type,threshold,sampling,volatility_threshold,v_bars_duration,\n",
    "                            barrier_conf,min_return,risk,sign_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vl.marker_plot(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,train_y=dg.preprocess(train)\n",
    "test_X,test_y=dg.preprocess(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Sharpe Ratio : 0.24374659930821482\n",
      "Test Sharpe Ratio : 0.2397809183742253\n"
     ]
    }
   ],
   "source": [
    "_,sr,_,_=met_ob.sharpe_ratio(labels_features)\n",
    "_,train_sr,_,_=met_ob.sharpe_ratio(train)\n",
    "_,test_sr,_,_=met_ob.sharpe_ratio(test)\n",
    "\n",
    "print(\"Train Sharpe Ratio :\",train_sr)\n",
    "print(\"Test Sharpe Ratio :\",test_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sharpe Ratio : 0.24196861695468128\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Sharpe Ratio :\",sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_defective(temp_df,df):\n",
    "    \n",
    "    labels=temp_df.label\n",
    "    \n",
    "    pos_labs=labels[df.label==1.0]\n",
    "    pos_labs[pos_labs==-1.0]=-2.0\n",
    "    pos_labs[pos_labs==0.0]=10.0\n",
    "    labels[pos_labs.index]=pos_labs\n",
    "    \n",
    "    neg_labs=labels[df.label==-1.0]\n",
    "    neg_labs[neg_labs==1.0]=2.0\n",
    "    neg_labs[neg_labs==0.0]=10.0\n",
    "    labels[neg_labs.index]=neg_labs\n",
    "    \n",
    "    zero_labs=labels[df.label==0.0]\n",
    "    zero_labs[zero_labs==1.0]=2.0\n",
    "    zero_labs[zero_labs==-1.0]=-2.0\n",
    "    labels[zero_labs.index]=zero_labs\n",
    "    \n",
    "    temp_df['label']=labels\n",
    "    \n",
    "    return temp_df\n",
    "\n",
    "def calc_sharp_ratio(clf,df):\n",
    "    preds=clf.predict(df)\n",
    "    temp_df = df.copy(deep=True)\n",
    "    temp_df['label']=preds\n",
    "    _,sr,_,_=met_ob.sharpe_ratio(temp_df)\n",
    "    \n",
    "    return sr,temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train:  1.0  test:  0.5344634103124446 s_r:  0.004933737960265892\n"
     ]
    }
   ],
   "source": [
    "filename = 'svm_no_balance.sav'\n",
    "try: \n",
    "    clf = joblib.load(filename)\n",
    "except (OSError, IOError) as e:\n",
    "    print(e)\n",
    "    clf = svm.SVC(kernel='rbf')\n",
    "    clf.fit(train_X, train_y,)\n",
    "    joblib.dump(clf,filename)\n",
    "    \n",
    "acc_train = clf.score(train_X, train_y)\n",
    "acc_test = clf.score(test_X, test_y)\n",
    "\n",
    "sr,temp_df=calc_sharp_ratio(clf,test_X)\n",
    "\n",
    "print( ' train: ',  acc_train, ' test: ',  acc_test, 's_r: ',sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Sharp Ratio: 0.004933737960265892\n"
     ]
    }
   ],
   "source": [
    "sr,temp_df=calc_sharp_ratio(clf,test_X)\n",
    "temp_df=find_defective(temp_df,test)\n",
    "vl.compare_labels(test,temp_df,True)\n",
    "print(\"Test Sharp Ratio:\",sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train:  1.0  test:  0.5344634103124446 s_r:  0.004933737960265892\n"
     ]
    }
   ],
   "source": [
    "filename = 'svm_balanced_classes.sav'\n",
    "try: \n",
    "    clf = joblib.load(filename)\n",
    "except (OSError, IOError) as e:\n",
    "    \n",
    "    print(e)\n",
    "    clf = svm.SVC(kernel='rbf',class_weight='balanced', C=1.0, random_state=0)\n",
    "    clf.fit(train_X, train_y)\n",
    "    joblib.dump(clf,filename)\n",
    "    \n",
    "acc_train = clf.score(train_X, train_y)\n",
    "acc_test = clf.score(test_X, test_y)\n",
    "\n",
    "sr,temp_df=calc_sharp_ratio(clf,test_X)\n",
    "\n",
    "print( ' train: ',  acc_train, ' test: ',  acc_test, 's_r: ',sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Sharp Ratio: 0.004933737960265892\n"
     ]
    }
   ],
   "source": [
    "sr,temp_df=calc_sharp_ratio(clf,test_X)\n",
    "temp_df=find_defective(temp_df,test)\n",
    "vl.compare_labels(test,temp_df,True)\n",
    "print(\"Test Sharp Ratio:\",sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(train_X),columns=train_X.columns)\n",
    "X_test = pd.DataFrame(scaler.fit_transform(test_X),columns=test_X.columns)\n",
    "\n",
    "X_train['label']=train.label\n",
    "train_scaled=X_train.copy()\n",
    "del X_train['label']\n",
    "\n",
    "X_test['label']=test.label\n",
    "test_scaled=X_test.copy()\n",
    "del X_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train:  0.6395554880518428  test:  0.6741480125214104 s_r:  0.004816968433623166\n"
     ]
    }
   ],
   "source": [
    "filename = 'svm_no_balance_scaled.sav'\n",
    "try: \n",
    "    clf = joblib.load(filename)\n",
    "except (OSError, IOError) as e:\n",
    "    print(e)\n",
    "    clf = svm.SVC(kernel='rbf')\n",
    "    clf.fit(X_train, train_y,)\n",
    "    joblib.dump(clf,filename)\n",
    "    \n",
    "acc_train = clf.score(X_train, train_y)\n",
    "acc_test = clf.score(X_test, test_y)\n",
    "\n",
    "sr,temp_df=calc_sharp_ratio(clf,X_test)\n",
    "\n",
    "print( ' train: ',  acc_train, ' test: ',  acc_test, 's_r: ',sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Sharp Ratio: 0.004816968433623166\n"
     ]
    }
   ],
   "source": [
    "sr,temp_df=calc_sharp_ratio(clf,X_test)\n",
    "temp_df=find_defective(temp_df,test_scaled)\n",
    "vl.compare_labels(test_scaled,temp_df,True)\n",
    "print(\"Test Sharp Ratio:\",sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train:  0.6264935196435804  test:  0.5762801960900124 s_r:  0.0037417165950409613\n"
     ]
    }
   ],
   "source": [
    "filename = 'svm_balanced_classes_scaled.sav'\n",
    "try: \n",
    "    clf = joblib.load(filename)\n",
    "except (OSError, IOError) as e:\n",
    "    print(e)\n",
    "    clf = svm.SVC(kernel='rbf',class_weight='balanced', C=1.0, random_state=0)\n",
    "    clf.fit(X_train, train_y)\n",
    "    joblib.dump(clf,filename)\n",
    "    \n",
    "acc_train = clf.score(X_train, train_y)\n",
    "acc_test = clf.score(X_test, test_y)\n",
    "\n",
    "sr,temp_df=calc_sharp_ratio(clf,X_test)\n",
    "\n",
    "print( ' train: ',  acc_train, ' test: ',  acc_test, 's_r: ',sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Sharp Ratio: 0.0037417165950409613\n"
     ]
    }
   ],
   "source": [
    "sr,temp_df=calc_sharp_ratio(clf,X_test)\n",
    "temp_df=find_defective(temp_df,test_scaled)\n",
    "vl.compare_labels(test_scaled,temp_df,True)\n",
    "print(\"Test Sharp Ratio:\",sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0     6144\n",
       " 10.0    3979\n",
       " 1.0     2361\n",
       " 2.0     2267\n",
       " 0.0     1252\n",
       "-2.0      928\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0    9049\n",
       " 1.0    4755\n",
       " 0.0    3127\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scaled.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pd.concat([X_train,X_test]).reset_index()\n",
    "del X['index']\n",
    "y=pd.concat([train_y,test_y]).reset_index()\n",
    "del y['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siba/.local/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning:\n",
      "\n",
      "The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "\n",
      "/home/siba/.local/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning:\n",
      "\n",
      "The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "\n",
      "/home/siba/.local/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning:\n",
      "\n",
      "The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "\n",
      "/home/siba/.local/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning:\n",
      "\n",
      "The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "\n",
      "/home/siba/.local/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning:\n",
      "\n",
      "The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is unable to overfit\n"
     ]
    }
   ],
   "source": [
    "metric = 'f1_micro'    \n",
    "an_ob.check_overfitting(clf, X, y, scoring=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "an_frame=X.copy()\n",
    "an_frame['label']=y\n",
    "an_frame['label_pred']=clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "an_ob.check_dataset_correlation(an_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is overfitting the training set\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=1, bootstrap=False,class_weight='balanced_subsample')\n",
    "an_ob.check_overfitting(clf, X_train, train_y, scoring=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>sma_10</th>\n",
       "      <th>sma_20</th>\n",
       "      <th>ema_10</th>\n",
       "      <th>ema_20</th>\n",
       "      <th>BB_15</th>\n",
       "      <th>rsi_15</th>\n",
       "      <th>williamsr_15</th>\n",
       "      <th>roc_15</th>\n",
       "      <th>adl_15</th>\n",
       "      <th>vpt_0</th>\n",
       "      <th>emv_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.803359</td>\n",
       "      <td>-0.798308</td>\n",
       "      <td>-0.802303</td>\n",
       "      <td>-0.799532</td>\n",
       "      <td>-0.801596</td>\n",
       "      <td>-0.799861</td>\n",
       "      <td>1.511174</td>\n",
       "      <td>-1.585620</td>\n",
       "      <td>0.390466</td>\n",
       "      <td>-0.400328</td>\n",
       "      <td>-0.012513</td>\n",
       "      <td>-0.032132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.803359</td>\n",
       "      <td>-0.798816</td>\n",
       "      <td>-0.801961</td>\n",
       "      <td>-0.800221</td>\n",
       "      <td>-0.801753</td>\n",
       "      <td>-0.801457</td>\n",
       "      <td>1.285101</td>\n",
       "      <td>-1.585620</td>\n",
       "      <td>-0.111914</td>\n",
       "      <td>-0.399700</td>\n",
       "      <td>-0.000730</td>\n",
       "      <td>-0.032885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.802062</td>\n",
       "      <td>-0.799086</td>\n",
       "      <td>-0.801554</td>\n",
       "      <td>-0.800548</td>\n",
       "      <td>-0.801772</td>\n",
       "      <td>-0.801463</td>\n",
       "      <td>0.225949</td>\n",
       "      <td>-0.353700</td>\n",
       "      <td>-0.084645</td>\n",
       "      <td>-0.398998</td>\n",
       "      <td>-0.000730</td>\n",
       "      <td>-0.032885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.797883</td>\n",
       "      <td>-0.798834</td>\n",
       "      <td>-0.800938</td>\n",
       "      <td>-0.800056</td>\n",
       "      <td>-0.801391</td>\n",
       "      <td>-0.801270</td>\n",
       "      <td>-0.199004</td>\n",
       "      <td>-1.047608</td>\n",
       "      <td>0.036893</td>\n",
       "      <td>-0.398290</td>\n",
       "      <td>-0.000672</td>\n",
       "      <td>2.217117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.797883</td>\n",
       "      <td>-0.799140</td>\n",
       "      <td>-0.800323</td>\n",
       "      <td>-0.799653</td>\n",
       "      <td>-0.801045</td>\n",
       "      <td>-0.801144</td>\n",
       "      <td>0.007462</td>\n",
       "      <td>0.306012</td>\n",
       "      <td>0.293585</td>\n",
       "      <td>-0.398985</td>\n",
       "      <td>-0.000730</td>\n",
       "      <td>2.217509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.794460</td>\n",
       "      <td>-0.798545</td>\n",
       "      <td>-0.799591</td>\n",
       "      <td>-0.798701</td>\n",
       "      <td>-0.800407</td>\n",
       "      <td>-0.800531</td>\n",
       "      <td>0.138540</td>\n",
       "      <td>-0.770012</td>\n",
       "      <td>0.198422</td>\n",
       "      <td>-0.399614</td>\n",
       "      <td>-0.000598</td>\n",
       "      <td>2.369777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.797181</td>\n",
       "      <td>-0.798582</td>\n",
       "      <td>-0.798939</td>\n",
       "      <td>-0.798417</td>\n",
       "      <td>-0.800088</td>\n",
       "      <td>-0.800412</td>\n",
       "      <td>0.051629</td>\n",
       "      <td>0.279789</td>\n",
       "      <td>0.097861</td>\n",
       "      <td>-0.400158</td>\n",
       "      <td>-0.000774</td>\n",
       "      <td>2.369790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.794526</td>\n",
       "      <td>-0.798354</td>\n",
       "      <td>-0.798557</td>\n",
       "      <td>-0.797702</td>\n",
       "      <td>-0.799547</td>\n",
       "      <td>-0.799755</td>\n",
       "      <td>0.259620</td>\n",
       "      <td>0.279789</td>\n",
       "      <td>0.326329</td>\n",
       "      <td>-0.399673</td>\n",
       "      <td>-0.000478</td>\n",
       "      <td>2.838289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.794526</td>\n",
       "      <td>-0.798125</td>\n",
       "      <td>-0.798219</td>\n",
       "      <td>-0.797117</td>\n",
       "      <td>-0.799058</td>\n",
       "      <td>-0.799213</td>\n",
       "      <td>0.441258</td>\n",
       "      <td>-0.366809</td>\n",
       "      <td>-0.018884</td>\n",
       "      <td>-0.399278</td>\n",
       "      <td>-0.000730</td>\n",
       "      <td>2.837570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.798738</td>\n",
       "      <td>-0.798317</td>\n",
       "      <td>-0.798201</td>\n",
       "      <td>-0.797404</td>\n",
       "      <td>-0.799016</td>\n",
       "      <td>-0.799687</td>\n",
       "      <td>-0.054065</td>\n",
       "      <td>-0.400598</td>\n",
       "      <td>0.065587</td>\n",
       "      <td>-0.398908</td>\n",
       "      <td>-0.000730</td>\n",
       "      <td>2.838759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.798866</td>\n",
       "      <td>-0.797868</td>\n",
       "      <td>-0.797984</td>\n",
       "      <td>-0.797662</td>\n",
       "      <td>-0.798991</td>\n",
       "      <td>-0.799702</td>\n",
       "      <td>0.095612</td>\n",
       "      <td>0.130973</td>\n",
       "      <td>-0.163977</td>\n",
       "      <td>-0.398589</td>\n",
       "      <td>-0.001446</td>\n",
       "      <td>2.838570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.796852</td>\n",
       "      <td>-0.797217</td>\n",
       "      <td>-0.797913</td>\n",
       "      <td>-0.797507</td>\n",
       "      <td>-0.798776</td>\n",
       "      <td>-0.799707</td>\n",
       "      <td>-0.375483</td>\n",
       "      <td>1.676811</td>\n",
       "      <td>-0.039721</td>\n",
       "      <td>-0.398341</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>2.840147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.790994</td>\n",
       "      <td>-0.796110</td>\n",
       "      <td>-0.797494</td>\n",
       "      <td>-0.796315</td>\n",
       "      <td>-0.798023</td>\n",
       "      <td>-0.798315</td>\n",
       "      <td>-0.102665</td>\n",
       "      <td>1.676811</td>\n",
       "      <td>0.321623</td>\n",
       "      <td>-0.398098</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>2.841917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.790994</td>\n",
       "      <td>-0.795421</td>\n",
       "      <td>-0.797023</td>\n",
       "      <td>-0.795339</td>\n",
       "      <td>-0.797342</td>\n",
       "      <td>-0.797140</td>\n",
       "      <td>0.380887</td>\n",
       "      <td>0.815111</td>\n",
       "      <td>0.321623</td>\n",
       "      <td>-0.398280</td>\n",
       "      <td>-0.000730</td>\n",
       "      <td>2.841917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.794499</td>\n",
       "      <td>-0.795082</td>\n",
       "      <td>-0.797007</td>\n",
       "      <td>-0.795179</td>\n",
       "      <td>-0.797060</td>\n",
       "      <td>-0.796862</td>\n",
       "      <td>0.481677</td>\n",
       "      <td>0.640945</td>\n",
       "      <td>0.512853</td>\n",
       "      <td>-0.398503</td>\n",
       "      <td>-0.000730</td>\n",
       "      <td>2.830758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.794499</td>\n",
       "      <td>-0.795086</td>\n",
       "      <td>-0.796712</td>\n",
       "      <td>-0.795047</td>\n",
       "      <td>-0.796805</td>\n",
       "      <td>-0.797002</td>\n",
       "      <td>0.470512</td>\n",
       "      <td>-0.253088</td>\n",
       "      <td>0.512853</td>\n",
       "      <td>-0.398503</td>\n",
       "      <td>-0.000730</td>\n",
       "      <td>2.830758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.795818</td>\n",
       "      <td>-0.794950</td>\n",
       "      <td>-0.796662</td>\n",
       "      <td>-0.795180</td>\n",
       "      <td>-0.796700</td>\n",
       "      <td>-0.797602</td>\n",
       "      <td>0.894309</td>\n",
       "      <td>-0.253088</td>\n",
       "      <td>0.349915</td>\n",
       "      <td>-0.398330</td>\n",
       "      <td>-0.000745</td>\n",
       "      <td>2.830758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.795818</td>\n",
       "      <td>-0.795079</td>\n",
       "      <td>-0.796612</td>\n",
       "      <td>-0.795288</td>\n",
       "      <td>-0.796605</td>\n",
       "      <td>-0.798218</td>\n",
       "      <td>0.600736</td>\n",
       "      <td>-0.531931</td>\n",
       "      <td>0.090174</td>\n",
       "      <td>-0.398232</td>\n",
       "      <td>-0.000730</td>\n",
       "      <td>0.580685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.797586</td>\n",
       "      <td>-0.795385</td>\n",
       "      <td>-0.796651</td>\n",
       "      <td>-0.795698</td>\n",
       "      <td>-0.796687</td>\n",
       "      <td>-0.798236</td>\n",
       "      <td>0.367898</td>\n",
       "      <td>-0.531931</td>\n",
       "      <td>-0.018968</td>\n",
       "      <td>-0.398320</td>\n",
       "      <td>-0.000730</td>\n",
       "      <td>0.334471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.797586</td>\n",
       "      <td>-0.795270</td>\n",
       "      <td>-0.796689</td>\n",
       "      <td>-0.796034</td>\n",
       "      <td>-0.796761</td>\n",
       "      <td>-0.798254</td>\n",
       "      <td>0.079143</td>\n",
       "      <td>-0.531931</td>\n",
       "      <td>-0.229666</td>\n",
       "      <td>-0.398397</td>\n",
       "      <td>-0.000730</td>\n",
       "      <td>0.182101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.797586</td>\n",
       "      <td>-0.795142</td>\n",
       "      <td>-0.796401</td>\n",
       "      <td>-0.796308</td>\n",
       "      <td>-0.796829</td>\n",
       "      <td>-0.798388</td>\n",
       "      <td>-0.147490</td>\n",
       "      <td>-0.531931</td>\n",
       "      <td>-0.062296</td>\n",
       "      <td>-0.398474</td>\n",
       "      <td>-0.000730</td>\n",
       "      <td>0.182101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.797586</td>\n",
       "      <td>-0.795215</td>\n",
       "      <td>-0.796112</td>\n",
       "      <td>-0.796533</td>\n",
       "      <td>-0.796890</td>\n",
       "      <td>-0.798377</td>\n",
       "      <td>-0.149758</td>\n",
       "      <td>-1.442410</td>\n",
       "      <td>-0.225599</td>\n",
       "      <td>-0.398551</td>\n",
       "      <td>-0.000730</td>\n",
       "      <td>-0.286882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.803356</td>\n",
       "      <td>-0.796452</td>\n",
       "      <td>-0.796176</td>\n",
       "      <td>-0.797766</td>\n",
       "      <td>-0.797495</td>\n",
       "      <td>-0.797673</td>\n",
       "      <td>-0.457786</td>\n",
       "      <td>-1.442410</td>\n",
       "      <td>-0.580761</td>\n",
       "      <td>-0.398628</td>\n",
       "      <td>-0.000730</td>\n",
       "      <td>-0.390823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.803356</td>\n",
       "      <td>-0.797688</td>\n",
       "      <td>-0.796450</td>\n",
       "      <td>-0.798775</td>\n",
       "      <td>-0.798042</td>\n",
       "      <td>-0.797398</td>\n",
       "      <td>-0.678833</td>\n",
       "      <td>-1.442050</td>\n",
       "      <td>-0.322780</td>\n",
       "      <td>-0.398705</td>\n",
       "      <td>-0.000730</td>\n",
       "      <td>-0.389974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.803354</td>\n",
       "      <td>-0.798574</td>\n",
       "      <td>-0.796724</td>\n",
       "      <td>-0.799600</td>\n",
       "      <td>-0.798537</td>\n",
       "      <td>-0.797013</td>\n",
       "      <td>-0.957443</td>\n",
       "      <td>-1.131462</td>\n",
       "      <td>-0.314761</td>\n",
       "      <td>-0.398782</td>\n",
       "      <td>-0.000730</td>\n",
       "      <td>-0.389779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.801386</td>\n",
       "      <td>-0.799263</td>\n",
       "      <td>-0.797071</td>\n",
       "      <td>-0.799918</td>\n",
       "      <td>-0.798798</td>\n",
       "      <td>-0.796928</td>\n",
       "      <td>-0.713489</td>\n",
       "      <td>-1.585620</td>\n",
       "      <td>-0.317016</td>\n",
       "      <td>-0.398858</td>\n",
       "      <td>-0.000690</td>\n",
       "      <td>-0.390690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.804264</td>\n",
       "      <td>-0.800108</td>\n",
       "      <td>-0.797425</td>\n",
       "      <td>-0.800701</td>\n",
       "      <td>-0.799308</td>\n",
       "      <td>-0.796698</td>\n",
       "      <td>-0.779486</td>\n",
       "      <td>-0.777102</td>\n",
       "      <td>-0.851124</td>\n",
       "      <td>-0.398935</td>\n",
       "      <td>-0.004315</td>\n",
       "      <td>-0.392459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.796049</td>\n",
       "      <td>-0.800131</td>\n",
       "      <td>-0.797501</td>\n",
       "      <td>-0.799847</td>\n",
       "      <td>-0.798986</td>\n",
       "      <td>-0.797801</td>\n",
       "      <td>-1.956689</td>\n",
       "      <td>0.009386</td>\n",
       "      <td>-0.347336</td>\n",
       "      <td>-0.399011</td>\n",
       "      <td>-0.000668</td>\n",
       "      <td>-0.375203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.789956</td>\n",
       "      <td>-0.799368</td>\n",
       "      <td>-0.797272</td>\n",
       "      <td>-0.798041</td>\n",
       "      <td>-0.798114</td>\n",
       "      <td>-0.797456</td>\n",
       "      <td>-0.772964</td>\n",
       "      <td>-1.585620</td>\n",
       "      <td>0.242268</td>\n",
       "      <td>-0.399054</td>\n",
       "      <td>0.005163</td>\n",
       "      <td>-0.357383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.793767</td>\n",
       "      <td>-0.798986</td>\n",
       "      <td>-0.797024</td>\n",
       "      <td>-0.797256</td>\n",
       "      <td>-0.797689</td>\n",
       "      <td>-0.797309</td>\n",
       "      <td>-0.148133</td>\n",
       "      <td>-1.455853</td>\n",
       "      <td>0.007704</td>\n",
       "      <td>-0.399007</td>\n",
       "      <td>-0.021470</td>\n",
       "      <td>-0.341149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56405</th>\n",
       "      <td>-1.215724</td>\n",
       "      <td>-1.216358</td>\n",
       "      <td>-1.217525</td>\n",
       "      <td>-1.216958</td>\n",
       "      <td>-1.217786</td>\n",
       "      <td>-1.252284</td>\n",
       "      <td>0.183533</td>\n",
       "      <td>-1.651989</td>\n",
       "      <td>0.011378</td>\n",
       "      <td>-2.743236</td>\n",
       "      <td>-0.008028</td>\n",
       "      <td>0.049104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56406</th>\n",
       "      <td>-1.216596</td>\n",
       "      <td>-1.216732</td>\n",
       "      <td>-1.217438</td>\n",
       "      <td>-1.217102</td>\n",
       "      <td>-1.217885</td>\n",
       "      <td>-1.252318</td>\n",
       "      <td>0.014112</td>\n",
       "      <td>-1.113516</td>\n",
       "      <td>-0.017860</td>\n",
       "      <td>-2.743209</td>\n",
       "      <td>-0.008218</td>\n",
       "      <td>0.042056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56407</th>\n",
       "      <td>-1.213557</td>\n",
       "      <td>-1.216783</td>\n",
       "      <td>-1.217134</td>\n",
       "      <td>-1.216668</td>\n",
       "      <td>-1.217685</td>\n",
       "      <td>-1.252245</td>\n",
       "      <td>-0.476345</td>\n",
       "      <td>-0.598856</td>\n",
       "      <td>0.063634</td>\n",
       "      <td>-2.742938</td>\n",
       "      <td>-0.007674</td>\n",
       "      <td>0.048599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56408</th>\n",
       "      <td>-1.210652</td>\n",
       "      <td>-1.216450</td>\n",
       "      <td>-1.216772</td>\n",
       "      <td>-1.215784</td>\n",
       "      <td>-1.217227</td>\n",
       "      <td>-1.251312</td>\n",
       "      <td>0.014789</td>\n",
       "      <td>-1.033629</td>\n",
       "      <td>0.064160</td>\n",
       "      <td>-2.742146</td>\n",
       "      <td>-0.007458</td>\n",
       "      <td>0.051675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56409</th>\n",
       "      <td>-1.213106</td>\n",
       "      <td>-1.216422</td>\n",
       "      <td>-1.216624</td>\n",
       "      <td>-1.215507</td>\n",
       "      <td>-1.217046</td>\n",
       "      <td>-1.251219</td>\n",
       "      <td>0.533344</td>\n",
       "      <td>-1.282445</td>\n",
       "      <td>0.026282</td>\n",
       "      <td>-2.742549</td>\n",
       "      <td>-0.009646</td>\n",
       "      <td>0.047608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56410</th>\n",
       "      <td>-1.212664</td>\n",
       "      <td>-1.215683</td>\n",
       "      <td>-1.216384</td>\n",
       "      <td>-1.215200</td>\n",
       "      <td>-1.216840</td>\n",
       "      <td>-1.251072</td>\n",
       "      <td>0.240425</td>\n",
       "      <td>-1.048828</td>\n",
       "      <td>0.022525</td>\n",
       "      <td>-2.742595</td>\n",
       "      <td>-0.007745</td>\n",
       "      <td>0.048117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56411</th>\n",
       "      <td>-1.211489</td>\n",
       "      <td>-1.215057</td>\n",
       "      <td>-1.216201</td>\n",
       "      <td>-1.214735</td>\n",
       "      <td>-1.216543</td>\n",
       "      <td>-1.250786</td>\n",
       "      <td>0.178803</td>\n",
       "      <td>0.499598</td>\n",
       "      <td>0.042368</td>\n",
       "      <td>-2.741110</td>\n",
       "      <td>-0.007629</td>\n",
       "      <td>0.050863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56412</th>\n",
       "      <td>-1.204452</td>\n",
       "      <td>-1.213810</td>\n",
       "      <td>-1.215725</td>\n",
       "      <td>-1.213075</td>\n",
       "      <td>-1.215603</td>\n",
       "      <td>-1.248021</td>\n",
       "      <td>0.171635</td>\n",
       "      <td>0.084791</td>\n",
       "      <td>0.157332</td>\n",
       "      <td>-2.740537</td>\n",
       "      <td>-0.005483</td>\n",
       "      <td>0.062994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56413</th>\n",
       "      <td>-1.209217</td>\n",
       "      <td>-1.213083</td>\n",
       "      <td>-1.215356</td>\n",
       "      <td>-1.212583</td>\n",
       "      <td>-1.215207</td>\n",
       "      <td>-1.247290</td>\n",
       "      <td>0.353265</td>\n",
       "      <td>-0.506310</td>\n",
       "      <td>0.079931</td>\n",
       "      <td>-2.740125</td>\n",
       "      <td>-0.009993</td>\n",
       "      <td>0.055255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56414</th>\n",
       "      <td>-1.212755</td>\n",
       "      <td>-1.212873</td>\n",
       "      <td>-1.215307</td>\n",
       "      <td>-1.212825</td>\n",
       "      <td>-1.215185</td>\n",
       "      <td>-1.247233</td>\n",
       "      <td>0.534551</td>\n",
       "      <td>0.178266</td>\n",
       "      <td>0.125466</td>\n",
       "      <td>-2.741237</td>\n",
       "      <td>-0.008403</td>\n",
       "      <td>0.056169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56415</th>\n",
       "      <td>-1.210275</td>\n",
       "      <td>-1.212328</td>\n",
       "      <td>-1.215143</td>\n",
       "      <td>-1.212571</td>\n",
       "      <td>-1.214929</td>\n",
       "      <td>-1.247235</td>\n",
       "      <td>0.846761</td>\n",
       "      <td>1.331992</td>\n",
       "      <td>0.127775</td>\n",
       "      <td>-2.742406</td>\n",
       "      <td>-0.006550</td>\n",
       "      <td>0.058264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56416</th>\n",
       "      <td>-1.206095</td>\n",
       "      <td>-1.211278</td>\n",
       "      <td>-1.214806</td>\n",
       "      <td>-1.211603</td>\n",
       "      <td>-1.214300</td>\n",
       "      <td>-1.246116</td>\n",
       "      <td>1.356700</td>\n",
       "      <td>0.651693</td>\n",
       "      <td>0.176317</td>\n",
       "      <td>-2.742384</td>\n",
       "      <td>-0.006757</td>\n",
       "      <td>0.059466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56417</th>\n",
       "      <td>-1.208584</td>\n",
       "      <td>-1.210781</td>\n",
       "      <td>-1.214582</td>\n",
       "      <td>-1.211264</td>\n",
       "      <td>-1.213967</td>\n",
       "      <td>-1.245795</td>\n",
       "      <td>1.262811</td>\n",
       "      <td>1.012974</td>\n",
       "      <td>0.134063</td>\n",
       "      <td>-2.743651</td>\n",
       "      <td>-0.008959</td>\n",
       "      <td>0.060273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56418</th>\n",
       "      <td>-1.208796</td>\n",
       "      <td>-1.210595</td>\n",
       "      <td>-1.214323</td>\n",
       "      <td>-1.211025</td>\n",
       "      <td>-1.213686</td>\n",
       "      <td>-1.245637</td>\n",
       "      <td>1.193316</td>\n",
       "      <td>1.567704</td>\n",
       "      <td>0.107264</td>\n",
       "      <td>-2.745499</td>\n",
       "      <td>-0.007911</td>\n",
       "      <td>0.057407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56419</th>\n",
       "      <td>-1.207916</td>\n",
       "      <td>-1.210076</td>\n",
       "      <td>-1.214050</td>\n",
       "      <td>-1.210670</td>\n",
       "      <td>-1.213349</td>\n",
       "      <td>-1.245285</td>\n",
       "      <td>0.968694</td>\n",
       "      <td>1.464170</td>\n",
       "      <td>0.132596</td>\n",
       "      <td>-2.744264</td>\n",
       "      <td>-0.007565</td>\n",
       "      <td>0.057939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56420</th>\n",
       "      <td>-1.208792</td>\n",
       "      <td>-1.209689</td>\n",
       "      <td>-1.213487</td>\n",
       "      <td>-1.210538</td>\n",
       "      <td>-1.213127</td>\n",
       "      <td>-1.245337</td>\n",
       "      <td>0.979470</td>\n",
       "      <td>1.311431</td>\n",
       "      <td>0.132563</td>\n",
       "      <td>-2.743034</td>\n",
       "      <td>-0.008020</td>\n",
       "      <td>0.061279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56421</th>\n",
       "      <td>-1.210084</td>\n",
       "      <td>-1.209549</td>\n",
       "      <td>-1.213104</td>\n",
       "      <td>-1.210666</td>\n",
       "      <td>-1.213049</td>\n",
       "      <td>-1.245939</td>\n",
       "      <td>1.303566</td>\n",
       "      <td>1.367299</td>\n",
       "      <td>0.069918</td>\n",
       "      <td>-2.743718</td>\n",
       "      <td>-0.008120</td>\n",
       "      <td>0.057004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56422</th>\n",
       "      <td>-1.209611</td>\n",
       "      <td>-1.210065</td>\n",
       "      <td>-1.212739</td>\n",
       "      <td>-1.210684</td>\n",
       "      <td>-1.212933</td>\n",
       "      <td>-1.246082</td>\n",
       "      <td>0.897285</td>\n",
       "      <td>1.645565</td>\n",
       "      <td>0.034783</td>\n",
       "      <td>-2.742008</td>\n",
       "      <td>-0.007710</td>\n",
       "      <td>0.051159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56423</th>\n",
       "      <td>-1.210054</td>\n",
       "      <td>-1.210148</td>\n",
       "      <td>-1.212417</td>\n",
       "      <td>-1.210779</td>\n",
       "      <td>-1.212871</td>\n",
       "      <td>-1.246069</td>\n",
       "      <td>-0.123674</td>\n",
       "      <td>1.310857</td>\n",
       "      <td>0.063839</td>\n",
       "      <td>-2.740815</td>\n",
       "      <td>-0.008039</td>\n",
       "      <td>0.053274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56424</th>\n",
       "      <td>-1.213123</td>\n",
       "      <td>-1.210185</td>\n",
       "      <td>-1.212330</td>\n",
       "      <td>-1.211415</td>\n",
       "      <td>-1.213107</td>\n",
       "      <td>-1.246067</td>\n",
       "      <td>0.310079</td>\n",
       "      <td>0.944003</td>\n",
       "      <td>0.013136</td>\n",
       "      <td>-2.741835</td>\n",
       "      <td>-0.009058</td>\n",
       "      <td>0.051022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56425</th>\n",
       "      <td>-1.216258</td>\n",
       "      <td>-1.210784</td>\n",
       "      <td>-1.212357</td>\n",
       "      <td>-1.212506</td>\n",
       "      <td>-1.213619</td>\n",
       "      <td>-1.245389</td>\n",
       "      <td>-0.178897</td>\n",
       "      <td>1.061254</td>\n",
       "      <td>-0.049052</td>\n",
       "      <td>-2.742528</td>\n",
       "      <td>-0.008415</td>\n",
       "      <td>0.046034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56426</th>\n",
       "      <td>-1.215724</td>\n",
       "      <td>-1.211746</td>\n",
       "      <td>-1.212313</td>\n",
       "      <td>-1.213301</td>\n",
       "      <td>-1.214031</td>\n",
       "      <td>-1.244956</td>\n",
       "      <td>-0.434968</td>\n",
       "      <td>1.704763</td>\n",
       "      <td>-0.142560</td>\n",
       "      <td>-2.741841</td>\n",
       "      <td>-0.007648</td>\n",
       "      <td>0.030716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56427</th>\n",
       "      <td>-1.211072</td>\n",
       "      <td>-1.211995</td>\n",
       "      <td>-1.212189</td>\n",
       "      <td>-1.213106</td>\n",
       "      <td>-1.213961</td>\n",
       "      <td>-1.246184</td>\n",
       "      <td>-1.265955</td>\n",
       "      <td>1.395581</td>\n",
       "      <td>-0.006990</td>\n",
       "      <td>-2.740183</td>\n",
       "      <td>-0.007085</td>\n",
       "      <td>0.047115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56428</th>\n",
       "      <td>-1.213544</td>\n",
       "      <td>-1.212470</td>\n",
       "      <td>-1.212334</td>\n",
       "      <td>-1.213395</td>\n",
       "      <td>-1.214133</td>\n",
       "      <td>-1.246323</td>\n",
       "      <td>-0.662047</td>\n",
       "      <td>1.704814</td>\n",
       "      <td>0.008379</td>\n",
       "      <td>-2.740533</td>\n",
       "      <td>-0.008250</td>\n",
       "      <td>0.051211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56429</th>\n",
       "      <td>-1.211935</td>\n",
       "      <td>-1.212872</td>\n",
       "      <td>-1.212275</td>\n",
       "      <td>-1.213340</td>\n",
       "      <td>-1.214136</td>\n",
       "      <td>-1.246330</td>\n",
       "      <td>-0.344349</td>\n",
       "      <td>1.323612</td>\n",
       "      <td>-0.004184</td>\n",
       "      <td>-2.741916</td>\n",
       "      <td>-0.007710</td>\n",
       "      <td>0.047703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56430</th>\n",
       "      <td>-1.214597</td>\n",
       "      <td>-1.213452</td>\n",
       "      <td>-1.212372</td>\n",
       "      <td>-1.213778</td>\n",
       "      <td>-1.214392</td>\n",
       "      <td>-1.246303</td>\n",
       "      <td>-0.471170</td>\n",
       "      <td>1.280308</td>\n",
       "      <td>-0.102725</td>\n",
       "      <td>-2.740689</td>\n",
       "      <td>-0.008158</td>\n",
       "      <td>0.045885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56431</th>\n",
       "      <td>-1.214983</td>\n",
       "      <td>-1.213942</td>\n",
       "      <td>-1.212547</td>\n",
       "      <td>-1.214207</td>\n",
       "      <td>-1.214660</td>\n",
       "      <td>-1.247243</td>\n",
       "      <td>-0.964255</td>\n",
       "      <td>0.041725</td>\n",
       "      <td>-0.072497</td>\n",
       "      <td>-2.738866</td>\n",
       "      <td>-0.007893</td>\n",
       "      <td>0.044441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56432</th>\n",
       "      <td>-1.217684</td>\n",
       "      <td>-1.216963</td>\n",
       "      <td>-1.214375</td>\n",
       "      <td>-1.217613</td>\n",
       "      <td>-1.216827</td>\n",
       "      <td>-1.246705</td>\n",
       "      <td>-1.692472</td>\n",
       "      <td>1.269012</td>\n",
       "      <td>-0.108450</td>\n",
       "      <td>-2.739101</td>\n",
       "      <td>-0.006897</td>\n",
       "      <td>0.043556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56433</th>\n",
       "      <td>-1.217992</td>\n",
       "      <td>-1.217136</td>\n",
       "      <td>-1.214760</td>\n",
       "      <td>-1.217892</td>\n",
       "      <td>-1.217150</td>\n",
       "      <td>-1.247641</td>\n",
       "      <td>-0.822660</td>\n",
       "      <td>0.713428</td>\n",
       "      <td>-0.094303</td>\n",
       "      <td>-2.740052</td>\n",
       "      <td>-0.007952</td>\n",
       "      <td>0.044300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56434</th>\n",
       "      <td>-1.221339</td>\n",
       "      <td>-1.220551</td>\n",
       "      <td>-1.217801</td>\n",
       "      <td>-1.220159</td>\n",
       "      <td>-1.219249</td>\n",
       "      <td>-1.251189</td>\n",
       "      <td>-0.615357</td>\n",
       "      <td>1.098840</td>\n",
       "      <td>-0.061356</td>\n",
       "      <td>-2.738528</td>\n",
       "      <td>-0.008105</td>\n",
       "      <td>0.046097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56435 rows  12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          close    sma_10    sma_20    ema_10    ema_20     BB_15    rsi_15  \\\n",
       "0     -0.803359 -0.798308 -0.802303 -0.799532 -0.801596 -0.799861  1.511174   \n",
       "1     -0.803359 -0.798816 -0.801961 -0.800221 -0.801753 -0.801457  1.285101   \n",
       "2     -0.802062 -0.799086 -0.801554 -0.800548 -0.801772 -0.801463  0.225949   \n",
       "3     -0.797883 -0.798834 -0.800938 -0.800056 -0.801391 -0.801270 -0.199004   \n",
       "4     -0.797883 -0.799140 -0.800323 -0.799653 -0.801045 -0.801144  0.007462   \n",
       "5     -0.794460 -0.798545 -0.799591 -0.798701 -0.800407 -0.800531  0.138540   \n",
       "6     -0.797181 -0.798582 -0.798939 -0.798417 -0.800088 -0.800412  0.051629   \n",
       "7     -0.794526 -0.798354 -0.798557 -0.797702 -0.799547 -0.799755  0.259620   \n",
       "8     -0.794526 -0.798125 -0.798219 -0.797117 -0.799058 -0.799213  0.441258   \n",
       "9     -0.798738 -0.798317 -0.798201 -0.797404 -0.799016 -0.799687 -0.054065   \n",
       "10    -0.798866 -0.797868 -0.797984 -0.797662 -0.798991 -0.799702  0.095612   \n",
       "11    -0.796852 -0.797217 -0.797913 -0.797507 -0.798776 -0.799707 -0.375483   \n",
       "12    -0.790994 -0.796110 -0.797494 -0.796315 -0.798023 -0.798315 -0.102665   \n",
       "13    -0.790994 -0.795421 -0.797023 -0.795339 -0.797342 -0.797140  0.380887   \n",
       "14    -0.794499 -0.795082 -0.797007 -0.795179 -0.797060 -0.796862  0.481677   \n",
       "15    -0.794499 -0.795086 -0.796712 -0.795047 -0.796805 -0.797002  0.470512   \n",
       "16    -0.795818 -0.794950 -0.796662 -0.795180 -0.796700 -0.797602  0.894309   \n",
       "17    -0.795818 -0.795079 -0.796612 -0.795288 -0.796605 -0.798218  0.600736   \n",
       "18    -0.797586 -0.795385 -0.796651 -0.795698 -0.796687 -0.798236  0.367898   \n",
       "19    -0.797586 -0.795270 -0.796689 -0.796034 -0.796761 -0.798254  0.079143   \n",
       "20    -0.797586 -0.795142 -0.796401 -0.796308 -0.796829 -0.798388 -0.147490   \n",
       "21    -0.797586 -0.795215 -0.796112 -0.796533 -0.796890 -0.798377 -0.149758   \n",
       "22    -0.803356 -0.796452 -0.796176 -0.797766 -0.797495 -0.797673 -0.457786   \n",
       "23    -0.803356 -0.797688 -0.796450 -0.798775 -0.798042 -0.797398 -0.678833   \n",
       "24    -0.803354 -0.798574 -0.796724 -0.799600 -0.798537 -0.797013 -0.957443   \n",
       "25    -0.801386 -0.799263 -0.797071 -0.799918 -0.798798 -0.796928 -0.713489   \n",
       "26    -0.804264 -0.800108 -0.797425 -0.800701 -0.799308 -0.796698 -0.779486   \n",
       "27    -0.796049 -0.800131 -0.797501 -0.799847 -0.798986 -0.797801 -1.956689   \n",
       "28    -0.789956 -0.799368 -0.797272 -0.798041 -0.798114 -0.797456 -0.772964   \n",
       "29    -0.793767 -0.798986 -0.797024 -0.797256 -0.797689 -0.797309 -0.148133   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "56405 -1.215724 -1.216358 -1.217525 -1.216958 -1.217786 -1.252284  0.183533   \n",
       "56406 -1.216596 -1.216732 -1.217438 -1.217102 -1.217885 -1.252318  0.014112   \n",
       "56407 -1.213557 -1.216783 -1.217134 -1.216668 -1.217685 -1.252245 -0.476345   \n",
       "56408 -1.210652 -1.216450 -1.216772 -1.215784 -1.217227 -1.251312  0.014789   \n",
       "56409 -1.213106 -1.216422 -1.216624 -1.215507 -1.217046 -1.251219  0.533344   \n",
       "56410 -1.212664 -1.215683 -1.216384 -1.215200 -1.216840 -1.251072  0.240425   \n",
       "56411 -1.211489 -1.215057 -1.216201 -1.214735 -1.216543 -1.250786  0.178803   \n",
       "56412 -1.204452 -1.213810 -1.215725 -1.213075 -1.215603 -1.248021  0.171635   \n",
       "56413 -1.209217 -1.213083 -1.215356 -1.212583 -1.215207 -1.247290  0.353265   \n",
       "56414 -1.212755 -1.212873 -1.215307 -1.212825 -1.215185 -1.247233  0.534551   \n",
       "56415 -1.210275 -1.212328 -1.215143 -1.212571 -1.214929 -1.247235  0.846761   \n",
       "56416 -1.206095 -1.211278 -1.214806 -1.211603 -1.214300 -1.246116  1.356700   \n",
       "56417 -1.208584 -1.210781 -1.214582 -1.211264 -1.213967 -1.245795  1.262811   \n",
       "56418 -1.208796 -1.210595 -1.214323 -1.211025 -1.213686 -1.245637  1.193316   \n",
       "56419 -1.207916 -1.210076 -1.214050 -1.210670 -1.213349 -1.245285  0.968694   \n",
       "56420 -1.208792 -1.209689 -1.213487 -1.210538 -1.213127 -1.245337  0.979470   \n",
       "56421 -1.210084 -1.209549 -1.213104 -1.210666 -1.213049 -1.245939  1.303566   \n",
       "56422 -1.209611 -1.210065 -1.212739 -1.210684 -1.212933 -1.246082  0.897285   \n",
       "56423 -1.210054 -1.210148 -1.212417 -1.210779 -1.212871 -1.246069 -0.123674   \n",
       "56424 -1.213123 -1.210185 -1.212330 -1.211415 -1.213107 -1.246067  0.310079   \n",
       "56425 -1.216258 -1.210784 -1.212357 -1.212506 -1.213619 -1.245389 -0.178897   \n",
       "56426 -1.215724 -1.211746 -1.212313 -1.213301 -1.214031 -1.244956 -0.434968   \n",
       "56427 -1.211072 -1.211995 -1.212189 -1.213106 -1.213961 -1.246184 -1.265955   \n",
       "56428 -1.213544 -1.212470 -1.212334 -1.213395 -1.214133 -1.246323 -0.662047   \n",
       "56429 -1.211935 -1.212872 -1.212275 -1.213340 -1.214136 -1.246330 -0.344349   \n",
       "56430 -1.214597 -1.213452 -1.212372 -1.213778 -1.214392 -1.246303 -0.471170   \n",
       "56431 -1.214983 -1.213942 -1.212547 -1.214207 -1.214660 -1.247243 -0.964255   \n",
       "56432 -1.217684 -1.216963 -1.214375 -1.217613 -1.216827 -1.246705 -1.692472   \n",
       "56433 -1.217992 -1.217136 -1.214760 -1.217892 -1.217150 -1.247641 -0.822660   \n",
       "56434 -1.221339 -1.220551 -1.217801 -1.220159 -1.219249 -1.251189 -0.615357   \n",
       "\n",
       "       williamsr_15    roc_15    adl_15     vpt_0     emv_0  \n",
       "0         -1.585620  0.390466 -0.400328 -0.012513 -0.032132  \n",
       "1         -1.585620 -0.111914 -0.399700 -0.000730 -0.032885  \n",
       "2         -0.353700 -0.084645 -0.398998 -0.000730 -0.032885  \n",
       "3         -1.047608  0.036893 -0.398290 -0.000672  2.217117  \n",
       "4          0.306012  0.293585 -0.398985 -0.000730  2.217509  \n",
       "5         -0.770012  0.198422 -0.399614 -0.000598  2.369777  \n",
       "6          0.279789  0.097861 -0.400158 -0.000774  2.369790  \n",
       "7          0.279789  0.326329 -0.399673 -0.000478  2.838289  \n",
       "8         -0.366809 -0.018884 -0.399278 -0.000730  2.837570  \n",
       "9         -0.400598  0.065587 -0.398908 -0.000730  2.838759  \n",
       "10         0.130973 -0.163977 -0.398589 -0.001446  2.838570  \n",
       "11         1.676811 -0.039721 -0.398341  0.000385  2.840147  \n",
       "12         1.676811  0.321623 -0.398098  0.011719  2.841917  \n",
       "13         0.815111  0.321623 -0.398280 -0.000730  2.841917  \n",
       "14         0.640945  0.512853 -0.398503 -0.000730  2.830758  \n",
       "15        -0.253088  0.512853 -0.398503 -0.000730  2.830758  \n",
       "16        -0.253088  0.349915 -0.398330 -0.000745  2.830758  \n",
       "17        -0.531931  0.090174 -0.398232 -0.000730  0.580685  \n",
       "18        -0.531931 -0.018968 -0.398320 -0.000730  0.334471  \n",
       "19        -0.531931 -0.229666 -0.398397 -0.000730  0.182101  \n",
       "20        -0.531931 -0.062296 -0.398474 -0.000730  0.182101  \n",
       "21        -1.442410 -0.225599 -0.398551 -0.000730 -0.286882  \n",
       "22        -1.442410 -0.580761 -0.398628 -0.000730 -0.390823  \n",
       "23        -1.442050 -0.322780 -0.398705 -0.000730 -0.389974  \n",
       "24        -1.131462 -0.314761 -0.398782 -0.000730 -0.389779  \n",
       "25        -1.585620 -0.317016 -0.398858 -0.000690 -0.390690  \n",
       "26        -0.777102 -0.851124 -0.398935 -0.004315 -0.392459  \n",
       "27         0.009386 -0.347336 -0.399011 -0.000668 -0.375203  \n",
       "28        -1.585620  0.242268 -0.399054  0.005163 -0.357383  \n",
       "29        -1.455853  0.007704 -0.399007 -0.021470 -0.341149  \n",
       "...             ...       ...       ...       ...       ...  \n",
       "56405     -1.651989  0.011378 -2.743236 -0.008028  0.049104  \n",
       "56406     -1.113516 -0.017860 -2.743209 -0.008218  0.042056  \n",
       "56407     -0.598856  0.063634 -2.742938 -0.007674  0.048599  \n",
       "56408     -1.033629  0.064160 -2.742146 -0.007458  0.051675  \n",
       "56409     -1.282445  0.026282 -2.742549 -0.009646  0.047608  \n",
       "56410     -1.048828  0.022525 -2.742595 -0.007745  0.048117  \n",
       "56411      0.499598  0.042368 -2.741110 -0.007629  0.050863  \n",
       "56412      0.084791  0.157332 -2.740537 -0.005483  0.062994  \n",
       "56413     -0.506310  0.079931 -2.740125 -0.009993  0.055255  \n",
       "56414      0.178266  0.125466 -2.741237 -0.008403  0.056169  \n",
       "56415      1.331992  0.127775 -2.742406 -0.006550  0.058264  \n",
       "56416      0.651693  0.176317 -2.742384 -0.006757  0.059466  \n",
       "56417      1.012974  0.134063 -2.743651 -0.008959  0.060273  \n",
       "56418      1.567704  0.107264 -2.745499 -0.007911  0.057407  \n",
       "56419      1.464170  0.132596 -2.744264 -0.007565  0.057939  \n",
       "56420      1.311431  0.132563 -2.743034 -0.008020  0.061279  \n",
       "56421      1.367299  0.069918 -2.743718 -0.008120  0.057004  \n",
       "56422      1.645565  0.034783 -2.742008 -0.007710  0.051159  \n",
       "56423      1.310857  0.063839 -2.740815 -0.008039  0.053274  \n",
       "56424      0.944003  0.013136 -2.741835 -0.009058  0.051022  \n",
       "56425      1.061254 -0.049052 -2.742528 -0.008415  0.046034  \n",
       "56426      1.704763 -0.142560 -2.741841 -0.007648  0.030716  \n",
       "56427      1.395581 -0.006990 -2.740183 -0.007085  0.047115  \n",
       "56428      1.704814  0.008379 -2.740533 -0.008250  0.051211  \n",
       "56429      1.323612 -0.004184 -2.741916 -0.007710  0.047703  \n",
       "56430      1.280308 -0.102725 -2.740689 -0.008158  0.045885  \n",
       "56431      0.041725 -0.072497 -2.738866 -0.007893  0.044441  \n",
       "56432      1.269012 -0.108450 -2.739101 -0.006897  0.043556  \n",
       "56433      0.713428 -0.094303 -2.740052 -0.007952  0.044300  \n",
       "56434      1.098840 -0.061356 -2.738528 -0.008105  0.046097  \n",
       "\n",
       "[56435 rows x 12 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      close    sma_10    sma_20    ema_10    ema_20     BB_15    rsi_15  \\\n",
      "0 -0.803359 -0.798308 -0.802303 -0.799532 -0.801596 -0.799861  1.511174   \n",
      "1 -0.803359 -0.798816 -0.801961 -0.800221 -0.801753 -0.801457  1.285101   \n",
      "2 -0.802062 -0.799086 -0.801554 -0.800548 -0.801772 -0.801463  0.225949   \n",
      "3 -0.797883 -0.798834 -0.800938 -0.800056 -0.801391 -0.801270 -0.199004   \n",
      "4 -0.797883 -0.799140 -0.800323 -0.799653 -0.801045 -0.801144  0.007462   \n",
      "\n",
      "   williamsr_15    roc_15    adl_15     vpt_0     emv_0  label  \n",
      "0     -1.585620  0.390466 -0.400328 -0.012513 -0.032132   -1.0  \n",
      "1     -1.585620 -0.111914 -0.399700 -0.000730 -0.032885    1.0  \n",
      "2     -0.353700 -0.084645 -0.398998 -0.000730 -0.032885    0.0  \n",
      "3     -1.047608  0.036893 -0.398290 -0.000672  2.217117    0.0  \n",
      "4      0.306012  0.293585 -0.398985 -0.000730  2.217509   -1.0  \n",
      "10  train:  0.7126873228027542  test:  0.6537711889433583  sharp_ratio: 0.0042783956733773885\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-8a4cfb5398df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'entropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_depth\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mclf_RF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0macc_train_RF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_RF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0macc_test_RF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_RF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    331\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 333\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    799\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    802\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    364\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tree_depth = 10\n",
    "num_estimator = [10, 20, 50, 100, 200, 300]\n",
    "avgU = 1.\n",
    "\n",
    "for i in num_estimator:\n",
    "    clf = tree.DecisionTreeClassifier(max_depth = tree_depth)\n",
    "    clf = RandomForestClassifier(n_estimators=i, criterion='entropy', max_depth = tree_depth,  max_features=None) \n",
    "    clf_RF = clf.fit(X_train, train_y)\n",
    "    acc_train_RF = clf_RF.score(X_train, train_y)\n",
    "    acc_test_RF = clf_RF.score(X_test, test_y)\n",
    "    \n",
    "    sr,temp_df=calc_sharp_ratio(clf,X)\n",
    "    \n",
    "    print(i, ' train: ',  acc_train_RF, ' test: ',  acc_test_RF,' sharp_ratio:',sr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
